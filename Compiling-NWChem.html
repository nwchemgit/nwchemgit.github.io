<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="canonical" href="https://nwchemgit.github.io/Compiling-NWChem.html">
  <link rel="shortcut icon" href="img/favicon.ico">
  <title>How to Compile NWChem - NWChem</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="css/theme.css" />
  <link rel="stylesheet" href="css/theme_extra.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "How to Compile NWChem";
    var mkdocs_page_input_path = "Compiling-NWChem.md";
    var mkdocs_page_url = "/Compiling-NWChem.html";
  </script>
  
  <script src="js/jquery-2.1.1.min.js" defer></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href="." class="icon icon-home"> NWChem</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="Home.html">NWChem Manual</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="Download.html">How to download and install NWChem</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="Quantum-Mechanical-Methods.html">Quantum Mechanical Methods</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="Quantum-Molecular-Dynamics.html">Quantum Molecular Dynamics</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="Hybrid-Approaches.html">Hybrid Approaches</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="Other-Capabilities.html">Other Capabilities</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="Examples.html">NWChem Examples</a>
                    </li>
                </ul>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="Compiling-NWChem.html">How to Compile NWChem</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#setting-up-the-proper-environment-variables">Setting up the proper environment variables</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#mpi-variables">MPI variables</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#automatic-detection-of-mpi-variables-with-mpif90">Automatic detection of MPI variables with mpif90</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#obsolete-how-to-se-the-mpi-variables">Obsolete:  How to se the MPI variables</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#how-to-start-nwchem">How to start NWChem</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#nwchem_modules">NWCHEM_MODULES</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#adding-optional-environmental-variables">Adding optional environmental variables</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#setting-python-environment-variables">Setting Python environment variables</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#optimized-math-libraries">Optimized math libraries</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#how-to-deal-with-integer-size-of-linear-algebra-libraries">How to deal with integer size of Linear Algebra libraries</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#linking-in-nbo">Linking in NBO</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#building-the-nwchem-binary">Building the NWChem binary</a>
    </li>
    </ul>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="Benchmarks.html">Benchmarks performed with NWChem</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="Known-Bugs.html">Known Bugs</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="FAQ.html">FAQ</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="Tutorials.html">NWChem Tutorials</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="Forum.html">Forum</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="Archived-Forum.html">Archived Forum</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="EMSL_Arrows.html">EMSL Arrows</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="Software-supporting-NWChem.html">Software supporting NWChem</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="Supplementary-Information.html">Supplementary Information</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="Containers.html">Containers</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href=".">NWChem</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".">Docs</a> &raquo;</li>
    
      
    
    <li>How to Compile NWChem</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/nwchemgit/nwchem-wiki/blob/master/Compiling-NWChem.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  
    <div class="rst-breadcrumbs-buttons" role="navigation" aria-label="breadcrumb navigation">
        <a href="Benchmarks.html" class="btn btn-neutral float-right" title="Benchmarks performed with NWChem">Next <span class="icon icon-circle-arrow-right"></span></a>
        <a href="Examples.html" class="btn btn-neutral" title="NWChem Examples"><span class="icon icon-circle-arrow-left"></span> Previous</a>
    </div>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="compiling-nwchem-from-source">Compiling NWChem from source<a class="headerlink" href="#compiling-nwchem-from-source" title="Permanent link">&para;</a></h1>
<p>On this page, a step-by-step description of the build process and
necessary and optional environment variables is outlined. In addition,
based on the experiences of developers and users how-to&rsquo;s for various
platforms have been created. These how-to&rsquo;s will be updated with
additional platforms and better environment variables over time.  </p>
<p>Download of the NWChem source is a step needed before compilation. Details for downloading
as well as instructions for installing pre-compiled version of NWChem are available at the
<a href="Download.html">Download page</a>.</p>
<h2 id="setting-up-the-proper-environment-variables">Setting up the proper environment variables<a class="headerlink" href="#setting-up-the-proper-environment-variables" title="Permanent link">&para;</a></h2>
<ul>
<li><code>$NWCHEM_TOP</code> defines the top directory of the NWChem source tree,
    e.g.</li>
</ul>
<p>When dealing with source from a <strong><em>NWChem release</em></strong> (6.8 in this example)</p>
<pre><code>export NWCHEM_TOP=&lt;your path&gt;/nwchem-6.8
</code></pre>

<ul>
<li><code>$NWCHEM_TARGET</code> defines your target platform, e.g.</li>
</ul>
<pre><code>export NWCHEM_TARGET=LINUX64
</code></pre>

<p>The following platforms are available:</p>
<table>
<thead>
<tr>
<th>NWCHEM_TARGET</th>
<th align="left">Platform</th>
<th>OS</th>
<th>Compilers</th>
</tr>
</thead>
<tbody>
<tr>
<td>LINUX</td>
<td align="left">x86</td>
<td>Linux</td>
<td>GNU, Intel, PGI</td>
</tr>
<tr>
<td></td>
<td align="left">ppc</td>
<td>Linux</td>
<td>GNU, IBM</td>
</tr>
<tr>
<td></td>
<td align="left">arm</td>
<td>Linux</td>
<td>GNU, flang</td>
</tr>
<tr>
<td>LINUX64</td>
<td align="left">x86_64</td>
<td>Linux</td>
<td>GNU, Intel, PGI, Flang</td>
</tr>
<tr>
<td></td>
<td align="left">ppc64le</td>
<td>Linux</td>
<td>GNU, IBM</td>
</tr>
<tr>
<td></td>
<td align="left">aarch64</td>
<td>Linux</td>
<td>GNU, flang</td>
</tr>
<tr>
<td>MACX</td>
<td align="left">x86</td>
<td>Darwin</td>
<td>GNU, Intel</td>
</tr>
<tr>
<td>MACX64</td>
<td align="left">x86_64</td>
<td>Darwin</td>
<td>GNU, Intel</td>
</tr>
<tr>
<td>BGL</td>
<td align="left">Blue Gene/L</td>
<td></td>
<td>IBM</td>
</tr>
<tr>
<td>BGP</td>
<td align="left">Blue Gene/P</td>
<td></td>
<td>IBM</td>
</tr>
<tr>
<td>BGQ</td>
<td align="left">Blue Gene/Q</td>
<td></td>
<td>IBM</td>
</tr>
</tbody>
</table>
<p><br></p>
<ul>
<li><code>$ARMCI_NETWORK</code> must be defined in order to achieve best
    performance on high performance networks, e.g.</li>
</ul>
<pre><code>export ARMCI_NETWORK=MPI-PR
</code></pre>

<p>For a single processor system, this environment variable does not have
to be defined.
Supported combination of ARMCI_NETWORK and NWCHEM_TARGET variables:  </p>
<table>
<thead>
<tr>
<th>ARMCI_NETWORK</th>
<th>NWCHEM_TARGET</th>
<th>Network</th>
<th>Protocol</th>
</tr>
</thead>
<tbody>
<tr>
<td>OPENIB</td>
<td>LINUX, LINUX64</td>
<td>Mellanox InfiniBand</td>
<td>Verbs</td>
</tr>
<tr>
<td>MPI-PR</td>
<td>LINUX64</td>
<td>Any network</td>
<td>MPI</td>
</tr>
<tr>
<td>MPI-MT<br />MPI-SPAWN</td>
<td>LINUX64</td>
<td>MPI supporting<br />multi-threading multiple</td>
<td>MPI-2</td>
</tr>
<tr>
<td>MPI-TS<br /> MPI-PT</td>
<td>any</td>
<td>any network with MPI</td>
<td>MPI</td>
</tr>
<tr>
<td>BGMLMPI</td>
<td>BGL</td>
<td>IBM Blue Gene/L</td>
<td>BGLMPI</td>
</tr>
<tr>
<td>DC        MFMPI</td>
<td>BGP</td>
<td>IBM Blue Gene/P</td>
<td>DCMF,MPI</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>Please see <a href="ARMCI.html">Choosing the ARMCI Library</a> for
additional information on choosing the right network
options.</p>
<h3 id="mpi-variables">MPI variables<a class="headerlink" href="#mpi-variables" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>USE_MPI</code></td>
<td>Set to &ldquo;y&rdquo; to indicate that NWChem should be compiled with MPI</td>
</tr>
<tr>
<td><code>USE_MPIF</code></td>
<td>Set to &ldquo;y&rdquo; for the NWPW module to use fortran-bindings of MPI. <br /> (Generally set when USE_MPI is set)</td>
</tr>
<tr>
<td><code>USE_MPIF4</code></td>
<td>Set to &ldquo;y&rdquo; for the NWPW module to use Integer*4 fortran-bindings of MPI. <br /> (Generally set when USE_MPI is set on most platforms)</td>
</tr>
<tr>
<td><code>LIBMPI</code></td>
<td>Name of the MPI library that should be linked with -l (eg. -lmpich)</td>
</tr>
<tr>
<td><code>MPI_LIB</code></td>
<td>Directory where the MPI library resides</td>
</tr>
<tr>
<td><code>MPI_INCLUDE</code></td>
<td>Directory where the MPI include files reside</td>
</tr>
</tbody>
</table>
<p><br></p>
<h4 id="automatic-detection-of-mpi-variables-with-mpif90"><span style="color:red;">Automatic detection of MPI variables with mpif90</span><a class="headerlink" href="#automatic-detection-of-mpi-variables-with-mpif90" title="Permanent link">&para;</a></h4>
<p><strong><em>New in NWChem 6.6</em></strong>: If the
location of the <code>mpif90</code> command is part of your <code>PATH</code> env. variable,
NWChem will figure out the values of <code>LIBMPI</code>, <code>MPI_LIB</code> and <code>MPI_INCLUDE</code>
(if they are not set). Therefore, we do <strong>NOT</strong> recommend to set <code>LIBMPI</code>,
<code>MPI_LIB</code> and <code>MPI_INCLUDE</code> and add the location of <code>mpif90</code> to the <code>PATH</code>
variable, instead. Therefore, the next section can be considered obsolete in the most common cases.</p>
<h4 id="obsolete-how-to-se-the-mpi-variables">Obsolete:  How to se the MPI variables<a class="headerlink" href="#obsolete-how-to-se-the-mpi-variables" title="Permanent link">&para;</a></h4>
<p>The output of the command</p>
<pre><code>mpif90 -show
</code></pre>

<p>can be used to extract the values of LIBMPI, MPI_LIB and MPI_INCLUDE</p>
<p>E.g. for MPICH2, this might look
like:</p>
<pre><code>$ mpif90 -show
f95 -I/usr/local/mpich2.141p1/include -I/usr/local/mpich2.141p1/include -L/usr/local/mpich2.141p1/lib \
-lmpichf90 -lmpichf90 -lmpich -lopa -lmpl -lrt -lpthread
</code></pre>

<p>The corresponding environment variables
are</p>
<pre><code>  % export USE_MPI=y
  % export LIBMPI=&quot;-lmpich -lopa -lmpl -lpthread -lmpichf90 -lfmpich -lmpich&quot;
  % export MPI_LIB=/usr/local/mpich2.141p1/lib 
  % export MPI_INCLUDE='/usr/local/mpich2.141p1/include
</code></pre>

<p>Note: a script is available since NWChem 6.5 to extract the environment variables listed above</p>
<p>$NWCHEM_TOP/contrib/distro-tools/getmpidefs_nwchem</p>
<table>
<caption>For some specific implementations the settings for MPI_LIB, MPI_INCLUDE, and LIBMPI look like:</caption>
<thead>
<tr>
<th>MPI Implementation</th>
<th>Environment variables</th>
</tr>
</thead>
<tbody>
<tr class="even">
<td>MPICH</td>
<td>export MPI_LOC=/usr/local #location of mpich installation<br />
export MPI_LIB=$MPI_LOC/lib<br />
export MPI_INCLUDE=$MPI_LOC/include<br />
export LIBMPI=&quot;-lfmpich -lmpich -lpmpich&quot;</td>
</tr>
<tr class="odd">
<td>MPICH2</td>
<td>export MPI_LOC=/usr/local #location of mpich2 installation<br />
export MPI_LIB=$MPI_LOC/lib<br />
export MPI_INCLUDE=$MPI_LOC/include<br />
export LIBMPI=&quot;-lmpich -lopa -lmpl -lrt -lpthread&quot;</td>
</tr>
<tr class="even">
<td>OPENMPI</td>
<td>export MPI_LOC=/usr/local #location of openmpi installation<br />
export MPI_LIB=$MPI_LOC/lib<br />
export MPI_INCLUDE=$MPI_LOC/include<br />
export LIBMPI=&quot;-lmpi_f90 -lmpi_f77 -lmpi -ldl -Wl,--export-dynamic -lnsl -lutil&quot;</td>
</tr>
</tbody>
</table>

<p><br></p>
<h4 id="how-to-start-nwchem">How to start NWChem<a class="headerlink" href="#how-to-start-nwchem" title="Permanent link">&para;</a></h4>
<p>When MPI is used, the appropriate MPI run command should be used to
start an NWChem calculation, e.g.</p>
<pre><code>  % mpirun -np 8 $NWCHEM_TOP/bin/$NWCHEM_TARGET/nwchem h2o.nw
</code></pre>

<h3 id="nwchem_modules">NWCHEM_MODULES<a class="headerlink" href="#nwchem_modules" title="Permanent link">&para;</a></h3>
<ul>
<li><code>$NWCHEM_MODULES</code> defines the modules to be compiled, e.g.</li>
</ul>
<pre><code>export NWCHEM_MODULES=&quot;all python&quot;
</code></pre>

<p>The following modules are available:</p>
<table>
<thead>
<tr>
<th>Module</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>all</td>
<td>Everything useful</td>
</tr>
<tr>
<td>all python</td>
<td>Everything useful plus python</td>
</tr>
<tr>
<td>qm</td>
<td>All quantum mechanics modules</td>
</tr>
<tr>
<td>md</td>
<td>MD only build</td>
</tr>
</tbody>
</table>
<p><br>
Note that additional environment variables need to be defined to specify
the location of the Python libraries, when the python module is
compiled. See the optional environmental variables section for
specifics.</p>
<h2 id="adding-optional-environmental-variables">Adding optional environmental variables<a class="headerlink" href="#adding-optional-environmental-variables" title="Permanent link">&para;</a></h2>
<p><strong>USE_NOFSCHECK</strong> can be set to avoid NWChem creating files for each
process when testing the size of the scratch directory (a.k.a. creation
of junk files), e.g.</p>
<pre><code>export USE_NOFSCHECK=TRUE
</code></pre>

<p><strong>USE_NOIO</strong> can be set to avoid NWChem 6.5 doing I/O for the ddscf,
mp2 and ccsd modules (it automatically sets <code>USE_NOFSCHECK</code>, too). It is
strongly recommended on large clusters or supercomputers or any computer
lacking any fast and large local filesystem.</p>
<pre><code>export USE_NOIO=TRUE
</code></pre>

<p><strong>LIB_DEFINES</strong> can be set to pass additional defines to the C
preprocessor (for both Fortran and C), e.g.</p>
<pre><code>export LIB_DEFINES=-DDFLT_TOT_MEM=16777216
</code></pre>

<p>Note: <code>-DDFLT_TOT_MEM</code> sets the default dynamic memory available for
NWChem to run, where the units are in doubles. Instead of manually
defining this environment variable, one can use the <a href="https://github.com/nwchemgit/nwchem/blob/master/contrib/getmem.nwchem"><code>getmem.nwchem</code></a> script in the
$NWCHEM_TOP/contrib directory. This script should be run after an
initial build of the binary has been completed. The script will choose the default memory settings based on the available physical memory, recompile the appropriate files and relink.</p>
<p><strong>MRCC_METHODS</strong> can be set to request the multireference coupled
cluster capability to be included in the code, e.g.</p>
<pre><code>export MRCC_METHODS=TRUE
</code></pre>

<p><strong>CCSDTQ</strong> can be set to request the CCSDTQ method and its derivatives 
to be included in the code, e.g.</p>
<pre><code>export CCSDTQ=TRUE
</code></pre>

<h3 id="setting-python-environment-variables">Setting Python environment variables<a class="headerlink" href="#setting-python-environment-variables" title="Permanent link">&para;</a></h3>
<p>Python programs may be embedded into the NWChem input and used to
control the execution of NWChem. To build with Python, Python needs to
be available on your machine. The software can be download from
<a href="https://www.python.org">https://www.python.org</a> . Follow the Python instructions for
installation and testing. NWChem has been tested with Python versions
up to 3.10</p>
<p>The following environment variables need to be set when compiling with
Python, together with having the location of your installed python binary part of
the <code>PATH</code> environment variable:</p>
<pre><code>export PYTHONVERSION=2.7
</code></pre>

<p>Note that the third number in the version should not be kept: 2.7.3
should be set as 2.7</p>
<p>You will also need to set PYTHONPATH to include any modules that you are
using in your input. Examples of Python within NWChem are in the
$NWCHEM_TOP/QA/tests/pyqa and $NWCHEM_TOP/contrib/python directories.</p>
<h3 id="optimized-math-libraries">Optimized math libraries<a class="headerlink" href="#optimized-math-libraries" title="Permanent link">&para;</a></h3>
<p>By default NWChem uses its own basic linear algebra subroutines (BLAS).
To include faster BLAS routines, the environment variable BLASOPT needs
to be set before building the code. For example, with
OpenBLAS</p>
<pre><code>export BLASOPT=&quot;-lopenblas&quot;
</code></pre>

<p>Good choices of optimized BLAS libraries on x86 (e.g. LINUX and LINUX64)
hardware include:  </p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>BLIS</td>
<td><a href="https://github.com/flame/blis">https://github.com/flame/blis</a></td>
</tr>
<tr>
<td>OpenBLAS</td>
<td><a href="https://github.com/xianyi/OpenBLAS">https://github.com/xianyi/OpenBLAS</a></td>
</tr>
<tr>
<td>GotoBLAS</td>
<td><a href="http://www.tacc.utexas.edu/tacc-projects/gotoblas2">http://www.tacc.utexas.edu/tacc-projects/gotoblas2</a></td>
</tr>
<tr>
<td>Intel MKL</td>
<td><a href="http://www.intel.com/software/products/mkl">http://www.intel.com/software/products/mkl</a></td>
</tr>
<tr>
<td>Cray LibSci</td>
<td>Available only on Cray hardware, it is automatically linked when compiling on Cray computers.</td>
</tr>
<tr>
<td>IBM ESSL</td>
<td>Available only on IBM hardware <a href="https://www.ibm.com/support/knowledgecenter/en/SSFHY8_6.3/navigation/welcome.html">https://www.ibm.com/support/knowledgecenter/en/SSFHY8_6.3/navigation/welcome.html</a></td>
</tr>
</tbody>
</table>
<p><br>
<em><strong>New since release 7.0.0 (after commit <a href="https://github.com/nwchemgit/nwchem/commit/6b0a971207e776f43dec81974014e86caf8cee61#diff-1750a4dcc9a0a9b1773d275e96c46a1e">6b0a971</a>)</strong></em>:  If BLASOPT is defined, the LAPACK_LIB environment variable must be set up, too.  LAPACK_LIB must provide the location of the library containing the LAPACK routines. For example, OpenBLAS provides the full suite of LAPACK routines, therefore, in this case, LAPACK_LIB can be set to the same value as BLASOPT</p>
<pre><code>export BLASOPT=-lopenblas
export LAPACK_LIB=-lopenblas
</code></pre>

<p>NWChem can also take advantage of the <a href="http://www.netlib.org/scalapack/">ScaLAPACK
library</a> if it is installed on your
system. The following environment variables need to be set:</p>
<pre><code>export USE_SCALAPACK=y

export SCALAPACK=&quot;location of Scalapack and BLACS library&quot;
</code></pre>

<h3 id="how-to-deal-with-integer-size-of-linear-algebra-libraries">How to deal with integer size of Linear Algebra libraries<a class="headerlink" href="#how-to-deal-with-integer-size-of-linear-algebra-libraries" title="Permanent link">&para;</a></h3>
<p>In the case of 64-bit platforms, most vendors optimized BLAS
libraries cannot be used. This is due to the fact that while NWChem uses
64-bit integers (i.e. integer*8) on 64-bit platforms, most of the
vendors optimized BLAS libraries used 32-bit integers. The same holds
for the ScaLAPACK libraries, which internally use 32-bit integers.<br />
The BLAS_SIZE environment variable is used at compile time to set the size of integer arguments in BLAS calls.  </p>
<table>
<thead>
<tr>
<th align="center">BLAS_SIZE</th>
<th>size of integer arguments in BLAS routines</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">4</td>
<td>32-bit (most common default)</td>
</tr>
<tr>
<td align="center">8</td>
<td>64-bit</td>
</tr>
</tbody>
</table>
<p>A method is available to link against the libraries mentioned above,
using the following procedure:</p>
<pre><code>   cd $NWCHEM_TOP/src
   make clean
   make 64_to_32
   make USE_64TO32=y  BLAS_SIZE=4 BLASOPT=&quot; optimized BLAS&quot; SCALAPACK=&quot;location of Scalapack and BLACS library&quot;
</code></pre>

<p>E.g., for IBM64 this looks like</p>
<pre><code>  % make USE_64TO32=y BLAS_SIZE=4 BLASOPT=&quot;-lessl -lmass&quot;
</code></pre>

<p>Notes:</p>
<ul>
<li>GotoBLAS2 (or OpenBLAS) can be installed with 64bit integers. This
    is accomplished by compiling the GotoBLAS2 library after having
    edited the GotoBLAS2 Makefile.rule file and un-commenting the line
    containing the INTERFACE64 definition. In other words, the line</li>
</ul>
<pre><code>          #INTERFACE64 = 1
</code></pre>

<pre><code>            needs to be changed to
</code></pre>
<pre><code>          INTERFACE64 = 1
</code></pre>

<ul>
<li>ACML and MKL can support 64-bit integers if the appropriate library
    is chosen. For MKL, one can choose the ILP64 Version of Intel® MKL
    and the correct recipe can be extracted from the website
    <a href="https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor">https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor</a>.
    For ACML the int64 libraries should be chosen, e.g. in the case of
    ACML 4.4.0 using a PGI compiler
    /opt/acml/4.4.0/pgi64_int64/lib/libacml.a</li>
</ul>
<h3 id="linking-in-nbo">Linking in NBO<a class="headerlink" href="#linking-in-nbo" title="Permanent link">&para;</a></h3>
<p>The 5.0 (obsolete) version of NBO provides a utility to generate source
code that can be linked into computational chemistry packages such as
NWChem. To utilize this functionality, follow the instructions in the
NBO 5 package to generate an nwnbo.f file. Linking NBO into NWChem can
be done using the following procedure:</p>
<pre><code>  % cd $NWCHEM_TOP/src 
  % cp nwnbo.f $NWCHEM_TOP/src/nbo/.  
  % make nwchem_config NWCHEM_MODULES=&quot;all nbo&quot; 
  % make
</code></pre>

<p>One can now use &ldquo;task nbo&rdquo; and incorporate NBO input into the NWChem
input file directly:</p>
<pre><code> nbo  
   $NBO NRT $END  
   ... 
 end  

 task nbo
</code></pre>

<h2 id="building-the-nwchem-binary">Building the NWChem binary<a class="headerlink" href="#building-the-nwchem-binary" title="Permanent link">&para;</a></h2>
<p>Once all required and optional environment variables have been set,
NWChem can be compiled:</p>
<pre><code>  % cd $NWCHEM_TOP/src  

  % make nwchem_config 

  % make &gt;&amp; make.log
</code></pre>

<p>The make above will use the standard compilers available on your system.
To use compilers different from the default one can either set
environment variables:</p>
<pre><code>  % export FC=&lt;fortran compiler&gt;  
  % export CC=&lt;c compiler&gt;
</code></pre>

<p>Or one can supply the compiler options to the make command (<em>recommended</em> option), e.g:</p>
<pre><code>  % make FC=ifort 
</code></pre>

<p>For example, on Linux FC could be set either equal to ifort, gfortran or pgf90</p>
<p><strong>Nota bene:</strong> NWChem does NOT support usage of the full path in FC and
CC variables. Please provide filenames only as in the examples above!</p>
<p>Note 1: If in a Linux environment, FC is set equal to anything other
than the tested compilers, there is no guarantee of a successful
installation, since the makefile structure has not been tested to
process other settings. In other words, please avoid make FC=&rdquo;ifort -O3 -xhost&rdquo;
and stick to make FC=&rdquo;ifort&rdquo;, instead</p>
<p>Note 2: It&rsquo;s better to avoid redefining CC, since a) NWChem does not
have C source that is a computational bottleneck and b) we typically
test just the default C compiler. In other words, the recommendation is
to compile with make FC=ifort</p>
<p>Note 3: It&rsquo;s better to avoid modifying the values of the FOPTIMIZE and
COPTIMIZE variables. The reason is that the default values for FOPTIMIZE
and COPTIMIZE have been tested by the NWChem developers (using the
internal QA suites, among others), while any modification might produce
incorrect results.</p>
<h1 id="how-to-linux-platforms">How-to: Linux platforms<a class="headerlink" href="#how-to-linux-platforms" title="Permanent link">&para;</a></h1>
<ul>
<li><strong>Common environmental variables for building in serial or in
    parallel with MPI</strong></li>
</ul>
<pre><code> % export NWCHEM_TOP=&lt;your path&gt;/nwchem  
 % export NWCHEM_TARGET=LINUX64  
 % export NWCHEM_MODULES =all
</code></pre>

<ul>
<li><strong>Common environmental variables for building with MPI</strong></li>
</ul>
<p>The following environment variables need to be set when NWChem is
compiled with
MPI:</p>
<pre><code>% export USE_MPI=y
% export USE_MPIF=y
% export USE_MPIF4=y

% export MPI_LOC=&lt;your path&gt;/openmpi-1.4.3  (for example, if you are using OpenMPI)
% export MPI_LIB=&lt;your path&gt;/openmpi-1.4.3/lib
% export MPI_INCLUDE=&lt;your path&gt;/openmpi-1.4.3/include
% export LIBMPI=&quot;-lmpi_f90 -lmpi_f77 -lmpi -lpthread&quot;
</code></pre>

<p><strong>New in NWChem 6.6:</strong> If the location of the mpif90 command is part of
your PATH env. variable, NWChem will figure out the values of LIBMPI,
MPI_LIB and MPI_INCLUDE (if they are not set). Therefore, we recommend
not to set LIBMPI, MPI_LIB and MPI_INCLUDE and add the location of
mpif90 to the PATH variable, instead.</p>
<ul>
<li><strong>Compiling the code once all variables are set</strong></li>
</ul>
<pre><code>% cd $NWCHEM_TOP/src
% make nwchem_config
% make FC=gfortran &gt;&amp; make.log
</code></pre>

<h4 id="nwchem-66-on-ubuntu-1404-trusty-tahr">NWChem 6.6 on Ubuntu 14.04 (Trusty Tahr)<a class="headerlink" href="#nwchem-66-on-ubuntu-1404-trusty-tahr" title="Permanent link">&para;</a></h4>
<p>These instruction are likely to work (with minor modifications) on all
Debian based distributions</p>
<ul>
<li>Packages
required:</li>
</ul>
<pre><code>python-dev gfortran libopenblas-dev libopenmpi-dev openmpi-bin tcsh make
</code></pre>

<ul>
<li>Settings</li>
</ul>
<pre><code>export USE_MPI=y
export NWCHEM_TARGET=LINUX64
export USE_PYTHONCONFIG=y
export PYTHONVERSION=2.7
export PYTHONHOME=/usr
export BLASOPT=&quot;-lopenblas -lpthread -lrt&quot;
export LAPACK_LIB=$BLASOPT
export BLAS_SIZE=4
export USE_64TO32=y
</code></pre>

<ul>
<li>Compilation steps</li>
</ul>
<pre><code>make nwchem_config NWCHEM_MODULES=&quot;all python&quot;
make 64_to_32
make
</code></pre>

<h4 id="nwchem-66-on-fedora-22">NWChem 6.6 on Fedora 22<a class="headerlink" href="#nwchem-66-on-fedora-22" title="Permanent link">&para;</a></h4>
<ul>
<li>Packages
required:</li>
</ul>
<pre><code>python-devel gcc-gfortran openblas-devel openblas-serial64 openmpi-devel tcsh make patch
</code></pre>

<ul>
<li>Settings</li>
</ul>
<pre><code>export USE_MPI=y
export NWCHEM_TARGET=LINUX64
export USE_PYTHONCONFIG=y
export PYTHONVERSION=2.7
export PYTHONHOME=/usr
export BLASOPT=&quot;-lnwclapack -lopenblas64&quot;
export BLAS_SIZE=8
export PATH=/usr/lib64/openmpi/bin:$PATH
export LD_LIBRARY_PATH=/usr/lib64/openmpi/lib:$LD_LIBRARY_PATH
export USE_ARUR=y
</code></pre>

<ul>
<li>Compilation steps</li>
</ul>
<pre><code>make nwchem_config NWCHEM_MODULES=&quot;all python&quot;
make
</code></pre>

<h4 id="nwchem-68-on-centos-71fedora-27">NWChem 6.8 on Centos 7.1/Fedora 27<a class="headerlink" href="#nwchem-68-on-centos-71fedora-27" title="Permanent link">&para;</a></h4>
<p>Once you have added the <a href="https://fedoraproject.org/wiki/EPEL">EPEL
repository</a> to your Centos/Fedora/RedHat
installation, you can have a more efficient NWChem build. </p>
<pre><code>sudo rpm -Uvh http://download.fedoraproject.org/pub/epel/7/x86_64/Packages/e/epel-release-7-11.noarch.rpm
</code></pre>

<ul>
<li>Packages
required:</li>
</ul>
<pre><code>python-devel gcc-gfortran openblas-devel openblas-serial64 openmpi-devel scalapack-openmpi-devel \
elpa-openmpi-devel tcsh openssh-clients which tar bzip2
</code></pre>

<ul>
<li>Settings</li>
</ul>
<pre><code>export USE_MPI=y
export NWCHEM_TARGET=LINUX64
export USE_PYTHONCONFIG=y
export PYTHONVERSION=2.7
export PYTHONHOME=/usr
export USE_64TO32=y
export BLAS_SIZE=4
export BLASOPT=&quot;-lopenblas -lpthread -lrt&quot;
export LAPACK_LIB=$BLASOPT
export SCALAPACK_SIZE=4
export SCALAPACK=&quot;-L/usr/lib64/openmpi/lib -lscalapack&quot;
export ELPA=&quot;-I/usr/lib64/gfortran/modules/openmpi -L/usr/lib64/openmpi/lib -lelpa&quot;
export LD_LIBRARY_PATH=/usr/lib64/openmpi/lib/:$LD_LIBRARY_PATH
export PATH=/usr/lib64/openmpi/bin/:$PATH
</code></pre>

<ul>
<li>Compilation steps</li>
</ul>
<pre><code>cd $NWCHEM_TOP/src  
make nwchem_config NWCHEM_MODULES=&quot;all python&quot;  
make 64_to_32  
make
</code></pre>

<h4 id="nwchem-66-on-redhat-6">NWChem 6.6 on RedHat 6<a class="headerlink" href="#nwchem-66-on-redhat-6" title="Permanent link">&para;</a></h4>
<ul>
<li>Packages required:</li>
</ul>
<pre><code>python-devel gcc-gfortran openmpi-devel tcsh make
</code></pre>

<ul>
<li>Settings</li>
</ul>
<pre><code>export USE_MPI=y
export NWCHEM_TARGET=LINUX64
export USE_PYTHONCONFIG=y
export PYTHONVERSION=2.6
export PYTHONHOME=/usr
export USE_INTERNALBLAS
export LD_LIBRARY_PATH=/usr/lib64/openmpi/lib/:$LD_LIBRARY_PATH
export PATH=/usr/lib64/openmpi/bin/:$PATH
</code></pre>

<ul>
<li>Compilation steps</li>
</ul>
<pre><code>make nwchem_config NWCHEM_MODULES=&quot;all python&quot;
make
</code></pre>

<h4 id="nwchem-66-on-redhat-6-epel-repository">NWChem 6.6 on RedHat 6 &amp; EPEL repository<a class="headerlink" href="#nwchem-66-on-redhat-6-epel-repository" title="Permanent link">&para;</a></h4>
<p>Once you have added the <a href="https://fedoraproject.org/wiki/EPEL">EPEL
repository</a> to you RedHat 6
installation, you can have a more efficient NWChem build. The settings
are exactly the same as
<a href="Compiling-NWChem.html#nwchem-68-on-centos-71fedora-27">Centos 7.1</a></p>
<h4 id="nwchem-66-on-opensuse-13">NWChem 6.6 on OpenSuse 13<a class="headerlink" href="#nwchem-66-on-opensuse-13" title="Permanent link">&para;</a></h4>
<ul>
<li>Packages
required:</li>
</ul>
<pre><code>gcc-fortran make python-devel openblas-devel openmpi-devel tcsh
</code></pre>

<ul>
<li>Settings</li>
</ul>
<pre><code>export USE_MPI=y
export NWCHEM_TARGET=LINUX64
export USE_PYTHONCONFIG=y
export PYTHONVERSION=2.7
export PYTHONHOME=/usr
export USE_64TO32=y
export BLAS_SIZE=4
export BLASOPT=&quot;-lopenblas -lpthread -lrt&quot;
export PATH=/usr/lib64/mpi/gcc/openmpi/bin:$PATH
export LD_LIBRARY_PATH=/usr/lib64/mpi/gcc/openmpi/lib64:$LD_LIBRARY_PATH
export PATH=/usr/lib64/openmpi/bin/:$PATH
</code></pre>

<ul>
<li>Compilation steps</li>
</ul>
<pre><code>make nwchem_config NWCHEM_MODULES=&quot;all python&quot;
make 64_to_32
make
</code></pre>

<h1 id="how-to-mac-platforms">How to: Mac platforms<a class="headerlink" href="#how-to-mac-platforms" title="Permanent link">&para;</a></h1>
<h2 id="compilation-of-nwchem-65-release-on-mac-os-x-109-x86_64">Compilation of NWChem 6.5 release on Mac OS X 10.9 x86_64<a class="headerlink" href="#compilation-of-nwchem-65-release-on-mac-os-x-109-x86_64" title="Permanent link">&para;</a></h2>
<ul>
<li>Download and unpack latest NWChem tarball to the directory of your
    choosing, say /Users/johndoe/nwchem</li>
<li>Install Homebrew as described at
<a href="http://brew.sh">http://brew.sh</a></li>
</ul>
<pre><code>ruby -e &quot;$(curl -fsSL https://raw.github.com/Homebrew/homebrew/go/install)&quot;
</code></pre>

<ul>
<li>Use Homebrew to install mpich2</li>
</ul>
<pre><code>brew install mpich2
</code></pre>

<ul>
<li>As usual, set the env. variables</li>
</ul>
<pre><code>export USE_MPI=y
export NWCHEM_MODULES=all
export NWCHEM_TARGET=MACX64
export NWCHEM_TOP=/Users/johndoe/nwchem
</code></pre>

<ul>
<li><strong>Important</strong>: set the following env. variable (GA will not compile
    otherwise)</li>
</ul>
<pre><code>export CFLAGS_FORGA=&quot;-DMPICH_NO_ATTR_TYPE_TAGS&quot;
</code></pre>

<ul>
<li>Go to your source directory, configure, and compile</li>
</ul>
<pre><code>cd /Users/johndoe/nwchem/src
make nwchem_config
make
</code></pre>

<h2 id="compilation-of-nwchem-66-on-mac-os-x-1010-yosemite-x86_64">Compilation of NWChem 6.6 on Mac OS X 10.10 (Yosemite) x86_64<a class="headerlink" href="#compilation-of-nwchem-66-on-mac-os-x-1010-yosemite-x86_64" title="Permanent link">&para;</a></h2>
<h4 id="method-1-using-gfortran-from-hpcsfnet-and-mpich-from-macports">Method #1: using gfortran from hpc.sf.net and mpich from macports<a class="headerlink" href="#method-1-using-gfortran-from-hpcsfnet-and-mpich-from-macports" title="Permanent link">&para;</a></h4>
<ul>
<li>Download and unpack latest NWChem tarball to the directory of your
    choosing, say /Users/johndoe/nwchem</li>
<li>Install gfortran (4.9) from <a href="http://hpc.sourceforge.net/">http://hpc.sourceforge.net/</a> (
    <a href="http://prdownloads.sourceforge.net/hpc/gcc-4.9-bin.tar.gz?download">http://prdownloads.sourceforge.net/hpc/gcc-4.9-bin.tar.gz?download</a>
    ) and make sure to add the location to your path</li>
</ul>
<!-- end list -->

<ul>
<li>Install mpi (e.g. using macports)</li>
</ul>
<pre><code>sudo port install mpich
sudo port select mpi mpich-mp-fortran
</code></pre>

<ul>
<li>Set environmental variables</li>
</ul>
<pre><code>export NWCHEM_TOP=/Users/johndoe/nwchem/
export NWCHEM_TARGET=MACX64
export USE_MPI=&quot;y&quot;
export USE_MPIF=&quot;y&quot;
export USE_MPIF4=&quot;y&quot;
export CFLAGS_FORGA=&quot;-DMPICH_NO_ATTR_TYPE_TAGS&quot;
export LIBMPI=&quot;-lmpifort -lmpi -lpmpi -lpthread&quot;
export BLASOPT=&quot; &quot;
</code></pre>

<ul>
<li>Go to your source directory, configure, and compile</li>
</ul>
<pre><code>cd /Users/johndoe/nwchem/src
make nwchem_config
make
</code></pre>

<h4 id="method-2-using-gfortran-and-openmpi-from-brew">Method #2: using gfortran and openmpi from brew<a class="headerlink" href="#method-2-using-gfortran-and-openmpi-from-brew" title="Permanent link">&para;</a></h4>
<ul>
<li>Download and unpack latest NWChem tarball to the directory of your
    choosing, say /Users/johndoe/nwchem</li>
<li>Install Homebrew as described at <a href="http://brew.sh">http://brew.sh</a> (more details at
    <a href="https://docs.brew.sh/Installation.html">https://docs.brew.sh/Installation.html</a>)</li>
</ul>
<!-- end list -->

<pre><code> /usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 
</code></pre>

<ul>
<li>Use Homebrew to install open-mpi</li>
</ul>
<pre><code>brew install open-mpi
</code></pre>

<ul>
<li>As usual, set the env. variables</li>
</ul>
<pre><code>export USE_MPI=y  
export NWCHEM_TARGET=MACX64  
export NWCHEM_TOP=/Users/johndoe/nwchem  
export USE_INTERNALBLAS=y
</code></pre>

<ul>
<li><strong>Important</strong>: set the following env. variable (GA will not compile
    otherwise)</li>
</ul>
<pre><code>export CFLAGS_FORGA &quot;-DMPICH_NO_ATTR_TYPE_TAGS&quot;
</code></pre>

<ul>
<li>Go to your source directory, configure, and compile</li>
</ul>
<pre><code> cd /Users/johndoe/nwchem/src  
 make nwchem_config`  
 make
</code></pre>

<p><strong>WARNING:</strong> Please do not use the Mac OS X default BLAS and LAPACK
libraries available (or brew&rsquo;s veclibfort), since they are causing
NWChem to produce erroneous results</p>
<h4 id="method-3-using-intel-compilers-and-mkl">Method #3: using Intel compilers and MKL<a class="headerlink" href="#method-3-using-intel-compilers-and-mkl" title="Permanent link">&para;</a></h4>
<p>The Intel compilers and MKL work just fine on Mac with the following
options:</p>
<pre><code>NWCHEM_TARGET=MACX64
CC=icc
FC=ifort
BLASOPT=&quot;-mkl -openmp&quot;
USE_OPENMP=T
</code></pre>

<p>MPICH and ARMCI-MPI work reliably on Mac. See <a href="ARMCI.html">Choosing the ARMCI
Library</a> for details on ARMCI-MPI</p>
<h1 id="how-to-cray-platforms">How-to: Cray platforms<a class="headerlink" href="#how-to-cray-platforms" title="Permanent link">&para;</a></h1>
<p>Common environmental variables for building and running on the Cray XT, XE, XC and XK:</p>
<pre><code>  % export NWCHEM_TOP=&lt;your path&gt;/nwchem  
  % export NWCHEM_TARGET=LINUX64  
  % export NWCHEM_MODULES=all  
  % export USE_MPI=y
  % export USE_MPIF=y 
  % export USE_MPIF4=y 
  % export USE_SCALAPACK=y 
  % export USE_64TO32=y  
  % export LIBMPI=&quot; &quot;
</code></pre>

<ul>
<li><strong>Compiling the code on Cray once all variables (described below)
    are set</strong></li>
</ul>
<pre><code>  % cd $NWCHEM_TOP/src
  % make nwchem_config
  % make 64_to_32
  % make FC=ftn &gt;&amp; make.log
</code></pre>

<p>The step <code>make 64_to_32</code> is required only if either SCALAPACK_SIZE or
BLAS_SIZE are set equal to 4.</p>
<h4 id="method-2-armci_networkmpi-pr">Method #2: ARMCI_NETWORK=MPI-PR<a class="headerlink" href="#method-2-armci_networkmpi-pr" title="Permanent link">&para;</a></h4>
<p>This is a <strong><span style="color:#FF0000">new option available in NWChem
6.6.</span></strong></p>
<p>Set the environmental variables for compilation:</p>
<pre><code>  % export ARMCI_NETWORK=MPI-PR
</code></pre>

<h4 id="example-olcf-titan">Example: OLCF Titan<a class="headerlink" href="#example-olcf-titan" title="Permanent link">&para;</a></h4>
<p>These are variables used for compilation on the <a href="https://www.olcf.ornl.gov/computing-resources/titan-cray-xk7/">OLCF Titan, a Cray
XK7</a> We
assume use of Portland Group compilers programming environment (<code>module
load PrgEnv-pgi</code>)</p>
<pre><code>NWCHEM_TARGET=LINUX64  
ARMCI_NETWORK=MPI-PR  
USE_64TO32=y  
USE_MPI=y  
BLAS_SIZE=4 
LAPACK_SIZE=4  
SCALAPACK_SIZE=4  
SCALAPACK=-lsci_pgi_mp  
BLASOPT=-lsci_pgi_mp
</code></pre>

<p>To enable the GPU part, set</p>
<pre><code>TCE_CUDA=y
</code></pre>

<p>and load the cudatoolkit module</p>
<pre><code>module load cudatoolkit
</code></pre>

<h3 id="aries-eg-xc30xc40">Aries, e.g. XC30/XC40<a class="headerlink" href="#aries-eg-xc30xc40" title="Permanent link">&para;</a></h3>
<h4 id="method-1-armci_networkmpi-pr">Method #1: ARMCI_NETWORK=MPI-PR<a class="headerlink" href="#method-1-armci_networkmpi-pr" title="Permanent link">&para;</a></h4>
<p>This is a <strong><span style="color:#FF0000">new option available in NWChem
6.6.</span></strong></p>
<p>Set the environmental variables for compilation:</p>
<pre><code>% export ARMCI_NETWORK=MPI-PR
</code></pre>

<h4 id="example-nersc-edison">Example: NERSC Edison<a class="headerlink" href="#example-nersc-edison" title="Permanent link">&para;</a></h4>
<p>These are variables used for compilation on <a href="https://www.nersc.gov/users/computational-systems/edison/">NERSC Edison, a Cray
XC30</a>, as of
October 23rd 2015, when using Intel compilers (i.e. after issuing the
commands <code>module swap PrgEnv-gnu PrgEnv-intel</code>). Very similar settings
can be applied to other Cray XC30 computers, such as <a href="http://www.archer.ac.uk">the UK ARCHER
computer</a></p>
<pre><code>CRAY_CPU_TARGET=sandybridge 
NWCHEM_TARGET=LINUX64  
ARMCI_NETWORK=MPI-PR  
USE_MPI=y
SCALAPACK=&quot;-L$MKLROOT/lib/intel64 -lmkl_scalapack_ilp64 -lmkl_intel_ilp64 -lmkl_core -lmkl_sequential \\  
-lmkl_blacs_intelmpi_ilp64 -lpthread -lm&quot;  
SCALAPACK_SIZE=8  
BLAS_SIZE=8  
BLASOPT=&quot;-L$MKLROOT/lib/intel64 -lmkl_intel_ilp64 -lmkl_core -lmkl_sequential -lpthread -lm&quot;  
LD_LIBRARY_PATH=/opt/gcc/4.9.2/snos/lib64:$LD_LIBRARY_PATH 
PATH=/opt/gcc/4.9.2/bin:$PATH  
CRAYPE_LINK_TYPE=dynamic 
</code></pre>

<p>To compile</p>
<pre><code>make nwchem_config 
make FC=ftn
</code></pre>

<p>The following env. variables needs to added to the batch queue
submission script   </p>
<pre><code>MPICH_GNI_MAX_VSHORT_MSG_SIZE=8192
MPICH_GNI_MAX_EAGER_MSG_SIZE=131027   
MPICH_GNI_NUM_BUFS=300   
MPICH_GNI_NDREG_MAXSIZE=16777216  
MPICH_GNI_MBOX_PLACEMENT=nic    
COMEX_MAX_NB_OUTSTANDING=6
</code></pre>

<h4 id="example-nersc-cori">Example: NERSC Cori<a class="headerlink" href="#example-nersc-cori" title="Permanent link">&para;</a></h4>
<p>These are variables used for compilation on the Haswell partition of
<a href="https://www.nersc.gov/users/computational-systems/cori">NERSC Edison, a Cray
XC40</a>, as of
November 6th 2016, when using Intel compilers (i.e. after issuing the
commands <code>module swap PrgEnv-gnu
PrgEnv-intel</code>).</p>
<pre><code>export NWCHEM_TARGET=LINUX64  
export USE_MPI=y  
export NWCHEM_TARGET=LINUX64  
export ARMCI_NETWORK=MPI-PR  
export USE_MPI=y  
export USE_SCALAPACK=y  
export SCALAPACK=&quot;-L$MKLROOT/lib/intel64 -lmkl_scalapack_ilp64 -lmkl_intel_ilp64 -lmkl_core -lmkl_sequential \  
-lmkl_blacs_intelmpi_ilp64 -lpthread -lm&quot;  
export SCALAPACK_SIZE=8  
export SCALAPACK_LIB=&quot;$SCALAPACK&quot; 
export BLAS_SIZE=8
export BLASOPT=&quot;-L$MKLROOT/lib/intel64 -lmkl_intel_ilp64 -lmkl_core -lmkl_sequential -lmkl_core -liomp5 -lpthread -ldmapp -lm&quot;  
export USE_NOIO=y  
export CRAYPE_LINK_TYPE=dynamic
</code></pre>

<p>To compile</p>
<pre><code>make nwchem_config
make FC=ftn
</code></pre>

<p>The following env. variables needs to added to the batch queue
submission script</p>
<pre><code>MPICH_GNI_MAX_VSHORT_MSG_SIZE=10000  
MPICH_GNI_MAX_EAGER_MSG_SIZE=98304  
MPICH_GNI_NUM_BUFS=300  
MPICH_GNI_NDREG_MAXSIZE=16777216 
MPICH_GNI_MBOX_PLACEMENT=nic
COMEX_MAX_NB_OUTSTANDING=6
</code></pre>

<h1 id="how-to-intel-xeon-phi">How-to: Intel Xeon Phi<a class="headerlink" href="#how-to-intel-xeon-phi" title="Permanent link">&para;</a></h1>
<p>This section describes both the newer KNL and older KNC hardware, in
reverse chronological order.</p>
<ul>
<li><strong>Compiling NWChem on self-hosted Intel Xeon Phi Knights Landing
    processors</strong></li>
</ul>
<p>NWChem 6.6 (and later versions) support OpenMP threading, which is
essential to obtaining good performance with NWChem on Intel Xeon Phi
many-core processors.<br />
As of November 2016, the development version of NWChem contains
threading support in the TCE coupled-cluster codes (primarily
non-iterative triples in e.g. CCSD(T)), semi-direct CCSD(T), and
plane-wave DFT (i.e. NWPW).</p>
<p>Required for compilation: Intel compilers, version 16+ (17+ is strongly
recommended).</p>
<p>Environmental variables required for compilation:</p>
<pre><code>% export USE_KNL=1 
% export USE_OPENMP=1  
% export USE_F90_ALLOCATABLE=T  
% export USE_FASTMEM=T
</code></pre>

<p>The latter two options are required to allocate temporaries in MCDRAM
when running in flat mode. Please do not use cache mode for NWChem
CCSD(T) codes. Note that using Fortran heap allocations means the memory
statistics generated by MA are no longer accurate, but we doubt that
anyone has been relying on these anyways.</p>
<p><code>USE_FASTMEM</code> requires the <em>memkind</em> library to be installed.
An open source version of the memkind library can be downloaded from <a href="https://github.com/memkind/memkind">Github</a></p>
<p>Side note: With the exception of <code>USE_FASTMEM</code>, all of the options in
the KNL section apply to Intel Xeon processors as well. OpenMP is
certainly useful on multicore processors as a way to reduce the
communication overhead and memory footprint of NWChem.</p>
<p>When using MKL and Intel 16+, please use the following
settings</p>
<pre><code>% export BLASOPT  =&quot;-mkl -qopenmp&quot; 
% export SCALAPACK=&quot;-mkl -qopenmp -lmkl_scalapack_ilp64 -lmkl_blacs_intelmpi_ilp64&quot;
</code></pre>

<p>The command require for compilation is</p>
<pre><code>$ make FC=ifort CC=icc
</code></pre>

<p>Environmental variables recommended at runtime (assuming Intel OpenMP
and MPI):</p>
<pre><code>% export I_MPI_PIN=1  
% export I_MPI_DEBUG=4  
% export KMP_BLOCKTIME=1 
% export KMP_AFFINITY=scatter,verbose
</code></pre>

<p>Once you are comfortable with the affinity settings, you can use these
instead:</p>
<pre><code>% export I_MPI_PIN=1
% export KMP_BLOCKTIME=1  
% export KMP_AFFINITY=scatter
</code></pre>

<p>Please consult the Intel or similar documentation regarding MPI+OpenMP
affinity on your system. This is a complicated issue that depends on the
software you use; it is impossible to document all the different
combinations of MPI and OpenMP implementations that may be used with
NWChem.</p>
<p>If you encounter segfaults not related to ARMCI, you may need to set the
following or recompile with <code>-heap-arrays</code>. Please create thread in the
Forum if you observe this.</p>
<pre><code>% ulimit -s unlimited  
% export OMP_STACKSIZE=32M
</code></pre>

<ul>
<li><strong>Compiling NWChem on hosts equipped with Intel Xeon Phi Knights
    Corner coprocessors</strong></li>
</ul>
<p>NWChem 6.5 (and later versions) offers the possibility of using Intel
Xeon Phi hardware to perform the most computationally intensive part of
the CCSD(T) calculations (non-iterative triples corrections).</p>
<p>Required for compilation: Intel Composer XE version 14.0.3 (or later
versions)</p>
<p>Environmental variables required for compilation:</p>
<pre><code>% export USE_OPENMP=1 
% export USE_OFFLOAD=1
</code></pre>

<p>When using MKL and Intel Composer XE version 14 (or later versions),
please use the following
settings</p>
<pre><code>% export BLASOPT  =&quot;-mkl -openmp   -lpthread -lm&quot;  
% export SCALAPACK=&quot;-mkl -openmp -lmkl_scalapack_ilp64 -lmkl_blacs_intelmpi_ilp64 -lpthread -lm&quot;
</code></pre>

<p>The command require for compilation is</p>
<pre><code>$ make FC=ifort 
</code></pre>

<ul>
<li><strong>Examples of recommended configurations</strong></li>
</ul>
<p>From our experience using the CCSD(T) TCE module, we have determined
that the optimal configuration is to use a single Global Arrays ranks
for offloading work to each Xeon Phi card.</p>
<p>On the EMSL cascade system, each node is equipped with two coprocessors,
and NWChem can allocate one GA ranks per coprocessor. In the job
scripts, we recommend spawning just 6 GA ranks for each node, instead of
16 (number that would match the number of physical cores). Therefore, 2
out 6 GA ranks assigned to a particular compute node will offload to the
coprocessors, while the remaining 6 cores while be used for traditional
CPU processing duties. Since during offload the host core is idle, we
can double the number of OpenMP threads for the host
(<code>OMP_NUM_THREADS=4</code>) in order to fill the idle core with work from
another GA rank (4 process with 4 threads each will total 16 threads on
each node).</p>
<p>NWChem itself automatically detects the available coprocessors in the
system and properly partitions them for optimal use, therefore no action
is required other than specifying the number of processes on each node
(using the appropriate mpirun/mpiexec options) and setting the value of
<code>OMP_NUM_THREADS</code> as in the example above.</p>
<p>Environmental variables useful at run-time:</p>
<p>OMP_NUM_THREADS is needed for the thread-level parallelization on the
Xeon CPU hosts</p>
<pre><code>% export OMP_NUM_THREADS=4
</code></pre>

<p>MIC_USE_2MB_BUFFER greatly improve communication between host and
Xeon Phi card</p>
<pre><code>% export MIC_USE_2MB_BUFFER=16K
</code></pre>

<p><strong>Very important</strong>: when running on
clusters equipped with Xeon Phi and Infiniband network hardware
(requiring <code>ARMCI_NETWORK=OPENIB</code>), the following env. variable is
required, even in the case when the Xeon Phi hardware is not
utilized.</p>
<pre><code>% export ARMCI_OPENIB_DEVICE=mlx4_0
</code></pre>

<h1 id="how-to-ibm-platforms">How-to: IBM platforms<a class="headerlink" href="#how-to-ibm-platforms" title="Permanent link">&para;</a></h1>
<ul>
<li><strong>Compiling NWChem on BLUEGENE/L</strong></li>
</ul>
<p>The following environment variables need to be
set</p>
<pre><code>% export NWCHEM_TOP=&lt;your path&gt;/nwchem
% export NWCHEM_TARGET=BGL
% export ARMCI_NETWORK=BGMLMPI
% export BGLSYS_DRIVER=/bgl/BlueLight/ppcfloor
% export BGLSYS_ROOT=${BGLSYS_DRIVER}/bglsys
% export BLRTS_GNU_ROOT=${BGLSYS_DRIVER}/blrts-gnu
% export BGDRIVER=${BGLSYS_DRIVER}
% export BGCOMPILERS=${BLRTS_GNU_ROOT}/bin
% export USE_MPI=y
% export LARGE_FILES=TRUE
% export MPI_LIB=${BGLSYS_ROOT}/lib
% export MPI_INCLUDE=${BGLSYS_ROOT}/include
% export LIBMPI=&quot;-lfmpich_.rts -lmpich.rts -lmsglayer.rts -lrts.rts -ldevices.rts&quot;
% export BGMLMPI_INCLUDE=/bgl/BlueLight/ppcfloor/bglsys/include
% export BGMLLIBS=/bgl/BlueLight/ppcfloor/bglsys/lib
</code></pre>

<p>To compile, the following commands should be used:</p>
<pre><code>% cd $NWCHEM_TOP/src
% make nwchem_config
% make FC=blrts_xlf &gt;&amp; make.log
</code></pre>

<ul>
<li><strong>Compiling NWChem on BLUEGENE/P</strong></li>
</ul>
<p>The following environment variables need to be
set</p>
<pre><code>% export NWCHEM_TARGET=BGP
% export ARMCI_NETWORK=DCMFMPI
% export MSG_COMMS=DCMFMPI
% export USE_MPI=y
% export LARGE_FILES=TRUE
% export BGP_INSTALLDIR=/bgsys/drivers/ppcfloor
% export BGCOMPILERS=/bgsys/drivers/ppcfloor/gnu-linux/bin
% export BGP_RUNTIMEPATH=/bgsys/drivers/ppcfloor/runtime
% export ARMCIDRV=${BGP_INSTALLDIR}
% export BGDRIVER=${ARMCIDRV}
% export MPI_LIB=${BGDRIVER}/comm/lib
% export MPI_INCLUDE=${BGDRIVER}/comm/include
% export LIBMPI=&quot;-L${MPI_LIB} -lfmpich_.cnk -lmpich.cnk -ldcmfcoll.cnk -ldcmf.cnk -lpthread -lrt -L${BGP_RUNTIMEPATH}/SPI -lSPI.cna&quot;
% export BGMLMPI_INCLUDE=${MPI_INCLUDE}
</code></pre>

<p>To compile, the following commands should be used:</p>
<pre><code>% cd $NWCHEM_TOP/src
% make nwchem_config
% make FC=bgxlf &gt;&amp; make.log
</code></pre>

<ul>
<li><strong>Compiling NWChem on BLUEGENE/Q</strong></li>
</ul>
<p>The following environment variables need to be
set</p>
<pre><code>% export NWCHEM_TARGET=BGQ
% export USE_MPI=y
% export USE_MPIF=y
% export USE_MPIF4=y
% export MPI_INCLUDE=/bgsys/drivers/ppcfloor/comm/xl/include
% export LIBMPI=&quot; &quot;
% export BLASOPT=&quot;/opt/ibmmath/essl/5.1/lib64/libesslbg.a -llapack -lblas -Wl,-zmuldefs &quot;
% export BLAS_LIB=&quot;/opt/ibmmath/essl/5.1/lib64/libesslbg.a -zmuldefs &quot;
% export BLAS_SIZE=4
% export USE_64TO32=y
% set path=(/bgsys/drivers/ppcfloor/gnu-linux/bin/ $path)
% export ARMCI_NETWORK=MPI-TS
% export DISABLE_GAMIRROR=y
</code></pre>

<p>To compile, the following commands should be used:</p>
<pre><code>% module load bgq-xl
% make nwchem_config
% make 64_to_32 &gt;&amp; make6t3.log
% make &gt;&amp; make.log
</code></pre>

<p><strong>WARNING</strong>: This is just a baseline port that we have tested and
validated against our QA suite. There is large room for improvement both
for serial performance (compiler options) and parallel performance (use
of alternative ARMCI_NETWORKs other than MPI-TS)</p>
<ul>
<li><strong>Compiling NWChem on IBM PowerPC architectures</strong></li>
</ul>
<p>The following environment variables should be set:</p>
<pre><code>% export NWCHEM_TOP=&lt;your path&gt;/nwchem
% export NWCHEM_TARGET=IBM64
% export ARMCI_NETWORK=MPI-MT
% export OBJECT_MODE=64
% export USE_MPI=y
% export LARGE_FILES=TRUE
% export MPI_LIB=/usr/lpp/ppe.poe/lib
% export MPI_INCLUDE=/usr/lpp/ppe.poe/include
% export LIBMPI=&quot;-lmpi -lpthreads&quot;
</code></pre>

<p>To compile, the following commands should be used:</p>
<pre><code>% cd $NWCHEM_TOP/src
% make nwchem_config
% make FC=xlf &gt;&amp; make.log
</code></pre>

<h1 id="how-to-commodity-clusters-with-infiniband">How-to: Commodity clusters with Infiniband<a class="headerlink" href="#how-to-commodity-clusters-with-infiniband" title="Permanent link">&para;</a></h1>
<p>Common environmental variables for building and running on most
Infiniband clusters are:</p>
<pre><code>  export NWCHEM_TOP=&lt;your path&gt;/nwchem  
  export NWCHEM_TARGET=LINUX64 
  export NWCHEM_MODULES=&quot;all&quot;  
  export USE_MPI=y 
  export USE_MPIF=y 
  export USE_MPIF4=y  
</code></pre>

<ul>
<li>On Infiniband clusters with the OpenIB software stack, the following
    environment variables should be defined</li>
</ul>
<pre><code>  export ARMCI_NETWORK=OPENIB 
  export IB_INCLUDE=&lt;Location of Infiniband libraries&gt;/include  
</code></pre>

<ul>
<li>Compiling the code on an Infiniband cluster once all variables are
    set</li>
</ul>
<pre><code>  cd $NWCHEM_TOP/src  

  make nwchem_config  

  make &gt;&amp; make.log
</code></pre>

<h1 id="how-to-commodity-clusters-with-intel-omni-path">How-to: Commodity clusters with Intel Omni-Path<a class="headerlink" href="#how-to-commodity-clusters-with-intel-omni-path" title="Permanent link">&para;</a></h1>
<ul>
<li>On  clusters with the Intel Omni-Path network, the following
    environment variables should be defined</li>
</ul>
<pre><code>  export ARMCI_NETWORK=MPI-PR
</code></pre>

<p>The following setting is needed to avoid run-time errors</p>
<pre><code>  export PSM2_MEMORY=large
</code></pre>

<p>More details on this topic discussed a  </p>
<ul>
<li>
<p><a href="https://github.com/nwchemgit/nwchem/issues/284">https://github.com/nwchemgit/nwchem/issues/284</a></p>
</li>
<li>
<p><a href="https://github.com/GlobalArrays/ga/issues/126">https://github.com/GlobalArrays/ga/issues/126</a></p>
</li>
</ul>
<h1 id="how-to-windows-platforms">How-to: Windows Platforms<a class="headerlink" href="#how-to-windows-platforms" title="Permanent link">&para;</a></h1>
<h2 id="mingw">MingW<a class="headerlink" href="#mingw" title="Permanent link">&para;</a></h2>
<p>The current recommended approach for building a NWChem binary for a
Windows platform is to build with the
<a href="http://www.mingw.org/">MinGW/Mingw32</a> environment. MinGW can be
installed using a semi-automatic tool mingw-get-setup.exe
(http://sourceforge.net/projects/mingw/files/Installer/). A basic MinGW
installation is required (Basic Setup), plus pthreads-32,
mingw32-gcc-fortran-dev of &ldquo;All Packages&rdquo; and the MSYS software.<br />
More detailed MinGW/MSYS installation tips can be found in the following
forum discussions</p>
<table>
<thead>
<tr>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://nwchemgit.github.io/Special_AWCforum/sp/id5124.html">https://nwchemgit.github.io/Special_AWCforum/sp/id5124.html</a></td>
</tr>
<tr>
<td><a href="https://nwchemgit.github.io/Special_AWCforum/sp/id6628.html">https://nwchemgit.github.io/Special_AWCforum/sp/id6628.html</a></td>
</tr>
</tbody>
</table>
<p>Another essential prerequisite step is to install Mpich, which can be
found at the following
URL</p>
<p><a href="http://www.mpich.org/static/tarballs/1.4.1p1/mpich2-1.4.1p1-win-ia32.msi">http://www.mpich.org/static/tarballs/1.4.1p1/mpich2-1.4.1p1-win-ia32.msi</a></p>
<p>Once Mpich is installed, you should copy the installation files to a
different location to avoid the failure of the tools compilation. You
can use the following command</p>
<pre><code>% cp -rp /c/Program\ Files\ \(x86\)/MPICH2/ ~/
</code></pre>

<p>You might want to install Python, too, by using the following
installation file</p>
<p><a href="https://www.python.org/ftp/python/2.7.8/python-2.7.8.msi">https://www.python.org/ftp/python/2.7.8/python-2.7.8.msi</a></p>
<p>Next, you need to set the env.</p>
<pre><code>% export NWCHEM_TOP=~/nwchem-6.8  
% export NWCHEM_TARGET=LINUX
% export USE_MPI=y  
% export MPI_LOC=~/MPICH2  
% export MPI_INCLUDE=$MPI_LOC/include  
% export MPI_LIB=$MPI_LOC/lib  
% export LIBMPI=&quot;-lfmpich2g -lmpi&quot;  
% export PYTHONVERSION=27  
% export DEPEND_CC=gcc
% export USE_INTERNALBLAS=y
% export NWCHEM_MODULES=all
</code></pre>

<p>Then, you can start the compilation by typing</p>
<pre><code>% cd $NWCHEM_TOP/src  
% make nwchem_config  
% make FC=gfortran DEPEND_CC=gcc
</code></pre>

<h3 id="msys2">MSYS2<a class="headerlink" href="#msys2" title="Permanent link">&para;</a></h3>
<p>https://github.com/msys2/msys2/wiki/MSYS2-installation</p>
<pre><code>pacman -Syuu
pacman -S mingw32/mingw-w64-i686-gcc-fortran
pacman -S mingw32/mingw-w64-i686-python3
pacman -S msys/make
</code></pre>

<h2 id="wsl-on-windows-10">WSL on Windows 10<a class="headerlink" href="#wsl-on-windows-10" title="Permanent link">&para;</a></h2>
<p>A good alternative only on Windows 10 is <strong>Windows Subsystem for Linux</strong> (WSL).
This option gives the best performance on Windows when <a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10">WLS 2</a> is used.
<strong>WSL</strong> allows you to obtain a functional command line Linux 64-bit NWChem environment, either by compiling the NWChem code from scratch or by using the Ubuntu precompiled NWChem package. Here is a link to the install guide</p>
<p><a href="https://msdn.microsoft.com/en-us/commandline/wsl/install_guide">https://msdn.microsoft.com/en-us/commandline/wsl/install_guide</a></p>
<p>Once Ubuntu is installed, the quickest method to install NWChem is by fetching the Ubuntu NWChem package by typing</p>
<pre><code>sudo apt install nwchem
</code></pre>

<h1 id="general-site-installation">General site installation<a class="headerlink" href="#general-site-installation" title="Permanent link">&para;</a></h1>
<p>The build procedures outlined above will allow use of NWChem within the
NWChem directory structure. The code will look for the basis set library
file in a default place within that directory structure. To install the
code in a general, public place (e.g., /usr/local/NWChem) the following
procedure can be applied:</p>
<ul>
<li>Determine the local storage path for the install files. (e.g.,
    /usr/local/NWChem).</li>
<li>Make directories</li>
</ul>
<pre><code>mkdir /usr/local/NWChem
mkdir /usr/local/NWChem/bin
mkdir /usr/local/NWChem/data
</code></pre>

<ul>
<li>Copy binary</li>
</ul>
<pre><code>cp $NWCHEM_TOP/bin/${NWCHEM_TARGET}/nwchem /usr/local/NWChem/bin
cd /usr/local/NWChem/bin
chmod 755 nwchem
</code></pre>

<ul>
<li>Set links to data files (basis sets, force fields, etc.)</li>
</ul>
<pre><code>cd $NWCHEM_TOP/src/basis
cp -r libraries /usr/local/NWChem/data
cd $NWCHEM_TOP/src/
cp -r data /usr/local/NWChem
cd $NWCHEM_TOP/src/nwpw
cp -r libraryps /usr/local/NWChem/data
</code></pre>

<ul>
<li>Each user will need a .nwchemrc file to point to these default data
    files. A global one could be put in /usr/local/NWChem/data and a
    symbolic link made in each users $HOME directory is probably the
    best plan for new installs. Users would have to issue the following
    command prior to using NWChem: ln -s
    /usr/local/NWChem/data/default.nwchemrc $HOME/.nwchemrc</li>
</ul>
<p>Contents of the default.nwchemrc file based on the above information
should be:</p>
<pre><code>nwchem_basis_library /usr/local/NWChem/data/libraries/
nwchem_nwpw_library /usr/local/NWChem/data/libraryps/
ffield amber
amber_1 /usr/local/NWChem/data/amber_s/
amber_2 /usr/local/NWChem/data/amber_q/
amber_3 /usr/local/NWChem/data/amber_x/
amber_4 /usr/local/NWChem/data/amber_u/
spce    /usr/local/NWChem/data/solvents/spce.rst
charmm_s /usr/local/NWChem/data/charmm_s/
charmm_x /usr/local/NWChem/data/charmm_x/
</code></pre>

<p>Of course users can copy this file instead of making the symbolic link
described above and change these defaults at their discretion.</p>
<p>It is can also be useful to use the NWCHEM_BASIS_LIBRARY environment
variable when testing a new installation when an old one exists. This
will allow you to overwrite the value of nwchem_basis_library in your
.nwchemrc file and point to the new basis library. For example:</p>
<pre><code>% export NWCHEM_BASIS_LIBRARY=&quot;$NWCHEM/data-5.0/libraries/&quot;
</code></pre>

<p>Do not forget the trailing &ldquo;/&rdquo;.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Benchmarks.html" class="btn btn-neutral float-right" title="Benchmarks performed with NWChem">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="Examples.html" class="btn btn-neutral" title="NWChem Examples"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/nwchemgit/nwchem-wiki/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="Examples.html" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="Benchmarks.html" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '.';</script>
    <script src="js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" defer></script>
      <script src="search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
